{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough of the Project: Wrangle and Analyze Data (WeRateDogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following report provides insight and a walktrough of the data wrangling process as I experienced it in Udacity's Wrangle and Analyze Data (WeRateDogs) project. I divided the report according to the main steps of data wrangling: gather, assess and clean, explaining my efforts at each stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather\n",
    "The objective of this stage of the data wrangling process is to get together all the data that you possibly might need for an specific project. There are three main methods to achieve this: download the data manually, download the data programmatically using a library, and downloading the data through an API.\n",
    "\n",
    "For this project, a link was provided to download a twitter archive manually. Additionally, a second link was provided to download its contents programmatically. I achieved this by using the Request library and applying its .get() method on the provided url and writing that data into a file. Lastly, I used Python's Tweepy library to query the Twitter API for each tweet's JSON data. Following the steps provided in twitter's website, I created a consumer_key and token which were used to generate an API and apply its .get_status() method on each tweet's id. This last two methods for downloading data are known as web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess\n",
    "The objective of this step is to evaluate the status of the data in terms of  quality and tidiness. Quality errors are also known as messy data. Basically, information that doesn't belong in the dataset, either because it is incomplete or because it has errors. Tidiness on the other hand, refers to issues regarding the structure of the data: \"each variable is a column, each observation is a row, and each type of observational unit is a table\". (Wickham, H. 2014)\n",
    "\n",
    "Some of the quality errors found in the data were the following: wrong datatype for columns like 'tweet_id' which was an integer when it should have been a string and 'timestamp' which was a string when it should have been a datetime object; rows with missing data for several columns like the dog classification or the extended url; rows with incorrect data like rows with the wrong dog name. \n",
    "\n",
    "Some of the tidiness issues found were the following: column headers were values and not variable names, which means there was a seperate column to determine if the dog is a puppo, a seperate column for doggo, yet another for floofer and pupper; all tables should be merged into one as they all represent one informational unit, information about tweets.\n",
    "\n",
    "There are two main ways to assess the data, visually and programmatically. The visual assessment is pretty straight forward, you just need to print all the data and be on the look for any issue. The programmatic assessment utilizes tools like the pandas libraries and its methods (.head(), .tail(), .info(), etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "The objective of this step is to fix the issues brought up during the assessment step. This is achieved by following a define-code-test loop. First you define the issue and what needs to be fixed, then you try to fix it, and finally, you test that the fixing was succesful.\n",
    "\n",
    "The first thing I did was to create copies of the dataframes to work on and proceeded to fix tidiness issues. \n",
    "\n",
    "Secondly, I merged the three tables into one by using the .merge() method. \n",
    "\n",
    "Then I created a new column labeled 'dog_stage' to be filled with the correct dog stage extracted from the tweet itself. I used the .str.extract() method along with regular expressions (regex) to extract the dog stage. With that already taken care of, I dropped the columns for 'doggo', 'floofer', 'pupper' and 'puppo' which are the seperate dog stages. \n",
    "\n",
    "After fixing those two tidiness issues, I proceeded to fix the quality issues. There were many retweets in the dataset that I needed to get rid of. I found out that if the expanded url of  a tweet didn't contain 'dog_rates' that meant the tweet was a retweet. I used the .str.contains() method to find all the retweets and to create a new dataframe with just the original tweets. \n",
    "\n",
    "I deleted unnecessary columns like 'in_reply_to_status_id' and 'in_reply_to_user_id' by using the .drop() method.\n",
    "\n",
    "Then I changed the 'tweet_id' datatype from integer to string by passing the column through the .astype() method. \n",
    "\n",
    "To make the tables easier to read, I applied the .set_option('display.max_colwdith', -1) method which shows the text in the table in its entirety.\n",
    "\n",
    "Next, I looked for the dog name and changed it in the column 'name'. Once again I utilized the .str.extract() method along with regex to get the names. \n",
    "\n",
    "I changed the values in the columns 'p1', 'p2', and 'p3' to start all of them with an uppercase letter. I used the .str.capitalize() method to achieve this. I did the same thing for the column 'dog_stage'.\n",
    "\n",
    "Some rows had a duplicated url inside the same cell in the 'expanded_urls' column. I erased the duplicates by using, once again, the .str.extract() method and regex.\n",
    "\n",
    "Finally, I changed the column 'timestamp' datatype from string to datetime by using the .to_datetime() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Through this project I gained some important skills for data wrangling. I learned about the different ways one can gather data, either manually or programmatically by using the request library or an api. Another important subject that now is really clear to me, is the understanding of the differences between quality issues and tidiness issues. Where one deals more with mistakes in the data, the other one focuses more on structural issues. I also got the chance to familiarize more with Pandas and all its tools to manipulate data, going from simple methods like .info() to more complicated ones that involved the use of carefully constructed regex. The most difficult part of the project was the cleaning part. Some issues were really easy clean, but some others were not, especially the ones that required regex. But in general, I gained a better understanding of the wrangling process, the kinds of issues that arise, ways of cleaning data and the tools to achieve all of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
